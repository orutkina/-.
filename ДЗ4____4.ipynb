{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orutkina/-./blob/main/%D0%94%D0%974____4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas seaborn scikit-learn"
      ],
      "metadata": {
        "id": "Uqrhf0yZYaYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание 4"
      ],
      "metadata": {
        "id": "YVOTcJhdYZgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install pandas seaborn scikit-learn matplotlib plotly\n",
        "\n",
        "# Импорт библиотек\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка отображения\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "xkfRUwTCZpwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 1: Загрузка и предварительная обработка данных"
      ],
      "metadata": {
        "id": "9vixXBu5YckG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGt-kON1YUur"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ЧАСТЬ 1: ЗАГРУЗКА И ПРЕДВАРИТЕЛЬНАЯ ОБРАБОТКА ДАННЫХ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Загрузка данных\n",
        "df = pd.read_csv('Sleep_health_and_lifestyle_dataset.csv')\n",
        "\n",
        "# Создаем копию для анализа (необработанные данные)\n",
        "df_not_processed = df.copy()\n",
        "\n",
        "print(f\"Размер исходного датасета: {df.shape}\")\n",
        "print(f\"Колонки: {df.columns.tolist()}\")\n",
        "\n",
        "# Просмотр первых строк\n",
        "print(\"\\nПервые 5 строк данных:\")\n",
        "print(df.head())\n",
        "\n",
        "# Информация о типах данных\n",
        "print(\"\\nИнформация о типах данных:\")\n",
        "print(df.info())\n",
        "\n",
        "# Проверка на пропущенные значения\n",
        "print(\"\\nПропущенные значения:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 2: Обработка категориальных признаков"
      ],
      "metadata": {
        "id": "Vuqg0WIAe5gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 2: ОБРАБОТКА КАТЕГОРИАЛЬНЫХ ПРИЗНАКОВ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Определяем категориальные признаки\n",
        "categorical_cols = ['Gender', 'Occupation', 'BMI Category', 'Sleep Disorder']\n",
        "\n",
        "# Преобразуем в тип category\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].astype('category')\n",
        "    df_not_processed[col] = df_not_processed[col].astype('category')\n",
        "\n",
        "print(\"Категориальные признаки преобразованы в тип 'category'\")\n",
        "print(f\"Категориальные колонки: {categorical_cols}\")\n",
        "\n",
        "# Применяем One-Hot Encoding к категориальным признакам\n",
        "print(\"\\nПрименяем One-Hot Encoding...\")\n",
        "\n",
        "# Используем pd.get_dummies с drop_first=True для избежания дамми-ловушки\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "print(f\"Размер датасета после One-Hot Encoding: {df_encoded.shape}\")\n",
        "print(f\"Новые колонки (первые 10): {df_encoded.columns.tolist()[:10]}\")\n",
        "print(f\"... и еще {len(df_encoded.columns.tolist()[10:])} колонок\")\n",
        "\n",
        "# Сохраняем имена числовых признаков для масштабирования\n",
        "numeric_features = df_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "print(f\"\\nЧисловые признаки для масштабирования ({len(numeric_features)}):\")\n",
        "print(numeric_features)"
      ],
      "metadata": {
        "id": "lS2kk5yKfB6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 3: Масштабирование данных"
      ],
      "metadata": {
        "id": "XK74qJc9fED6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 3: МАСШТАБИРОВАНИЕ ДАННЫХ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Создаем копию для масштабирования\n",
        "df_scaled = df_encoded.copy()\n",
        "\n",
        "# Используем StandardScaler для масштабирования (центрирование и приведение к единичной дисперсии)\n",
        "scaler = StandardScaler()\n",
        "df_scaled[numeric_features] = scaler.fit_transform(df_scaled[numeric_features])\n",
        "\n",
        "print(\"Данные масштабированы с использованием StandardScaler\")\n",
        "print(\"\\nСтатистика после масштабирования (первые 5 строк):\")\n",
        "print(df_scaled[numeric_features].head())\n",
        "\n",
        "# Также создадим версию с MinMaxScaler для сравнения\n",
        "df_minmax = df_encoded.copy()\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df_minmax[numeric_features] = minmax_scaler.fit_transform(df_minmax[numeric_features])\n",
        "\n",
        "print(\"\\nДля сравнения создана также версия с MinMaxScaler\")\n",
        "\n",
        "# Для кластеризации будем использовать StandardScaler, так как он лучше подходит для алгоритмов,\n",
        "# основанных на расстояниях (K-means, DBSCAN)"
      ],
      "metadata": {
        "id": "ezsEguwvfHNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 4: K-means кластеризация"
      ],
      "metadata": {
        "id": "PtEFjGuPfJMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 4: K-MEANS КЛАСТЕРИЗАЦИЯ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Выбираем данные для кластеризации (только числовые признаки)\n",
        "X = df_scaled[numeric_features].values\n",
        "\n",
        "print(f\"Размерность данных для кластеризации: {X.shape}\")\n",
        "print(f\"Количество признаков: {X.shape[1]}\")\n",
        "\n",
        "# Сначала попробуем K-means с 3 кластерами (исходя из EDA в предыдущих заданиях)\n",
        "kmeans_3 = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters_3 = kmeans_3.fit_predict(X)\n",
        "\n",
        "# Добавляем кластеры в исходный датасет\n",
        "df_not_processed['cluster_kmeans_3'] = clusters_3\n",
        "df_scaled['cluster_kmeans_3'] = clusters_3\n",
        "\n",
        "print(\"K-means кластеризация с 3 кластерами выполнена\")\n",
        "print(f\"Метки кластеров: {np.unique(clusters_3)}\")\n",
        "print(f\"Размеры кластеров:\")\n",
        "for i in range(3):\n",
        "    count = np.sum(clusters_3 == i)\n",
        "    print(f\"  Кластер {i}: {count} наблюдений ({count/len(clusters_3)*100:.1f}%)\")\n",
        "\n",
        "# Вычисляем метрики качества\n",
        "silhouette_3 = silhouette_score(X, clusters_3)\n",
        "calinski_3 = calinski_harabasz_score(X, clusters_3)\n",
        "davies_3 = davies_bouldin_score(X, clusters_3)\n",
        "\n",
        "print(\"\\nМетрики качества для 3 кластеров:\")\n",
        "print(f\"  Silhouette Score: {silhouette_3:.4f} (чем ближе к 1, тем лучше)\")\n",
        "print(f\"  Calinski-Harabasz Index: {calinski_3:.2f} (чем выше, тем лучше)\")\n",
        "print(f\"  Davies-Bouldin Index: {davies_3:.4f} (чем ближе к 0, тем лучше)\")"
      ],
      "metadata": {
        "id": "KYPoQK4pfM45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 5: Визуализация K-means кластеризации"
      ],
      "metadata": {
        "id": "twGI8ydJfO-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 5: ВИЗУАЛИЗАЦИЯ K-MEANS КЛАСТЕРИЗАЦИИ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Для визуализации используем PCA для уменьшения размерности до 2D\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(f\"Объясненная дисперсия PCA: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Суммарная объясненная дисперсия: {sum(pca.explained_variance_ratio_):.3f}\")\n",
        "\n",
        "# Создаем графики\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Визуализация кластеров в пространстве PCA\n",
        "scatter = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_3, cmap='viridis', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('PCA Component 1')\n",
        "axes[0, 0].set_ylabel('PCA Component 2')\n",
        "axes[0, 0].set_title('K-means (3 кластера) - PCA проекция')\n",
        "plt.colorbar(scatter, ax=axes[0, 0])\n",
        "\n",
        "# 2. Центроиды кластеров\n",
        "centroids_pca = pca.transform(kmeans_3.cluster_centers_)\n",
        "axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_3, cmap='viridis', alpha=0.3)\n",
        "axes[0, 1].scatter(centroids_pca[:, 0], centroids_pca[:, 1],\n",
        "                   marker='X', s=200, c='red', label='Центроиды')\n",
        "axes[0, 1].set_xlabel('PCA Component 1')\n",
        "axes[0, 1].set_ylabel('PCA Component 2')\n",
        "axes[0, 1].set_title('Центроиды кластеров')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Распределение кластеров\n",
        "cluster_counts = df_not_processed['cluster_kmeans_3'].value_counts().sort_index()\n",
        "bars = axes[0, 2].bar(cluster_counts.index, cluster_counts.values)\n",
        "axes[0, 2].set_xlabel('Кластер')\n",
        "axes[0, 2].set_ylabel('Количество наблюдений')\n",
        "axes[0, 2].set_title('Распределение по кластерам')\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                   f'{height}', ha='center', va='bottom')\n",
        "\n",
        "# 4. Анализ важных признаков для кластеров\n",
        "# Вычисляем средние значения признаков по кластерам\n",
        "cluster_means = df_scaled.groupby('cluster_kmeans_3')[numeric_features].mean()\n",
        "\n",
        "# Выбираем топ-5 признаков с наибольшей вариацией между кластерами\n",
        "feature_variation = cluster_means.std().sort_values(ascending=False).head(5)\n",
        "top_features = feature_variation.index.tolist()\n",
        "\n",
        "# Визуализируем средние значения топ-признаков по кластерам\n",
        "x = np.arange(len(top_features))\n",
        "width = 0.25\n",
        "\n",
        "for i in range(3):\n",
        "    values = cluster_means.loc[i, top_features].values\n",
        "    axes[1, 0].bar(x + i*width - width, values, width, label=f'Кластер {i}')\n",
        "\n",
        "axes[1, 0].set_xlabel('Признаки')\n",
        "axes[1, 0].set_ylabel('Среднее значение (стандартизированное)')\n",
        "axes[1, 0].set_title('Средние значения топ-5 признаков по кластерам')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(top_features, rotation=45, ha='right')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 5. Визуализация с t-SNE (альтернатива PCA)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "scatter_tsne = axes[1, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters_3, cmap='viridis', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('t-SNE Component 1')\n",
        "axes[1, 1].set_ylabel('t-SNE Component 2')\n",
        "axes[1, 1].set_title('K-means (3 кластера) - t-SNE проекция')\n",
        "plt.colorbar(scatter_tsne, ax=axes[1, 1])\n",
        "\n",
        "# 6. Анализ распределения исходных категориальных признаков по кластерам\n",
        "# Например, распределение по полу\n",
        "gender_dist = pd.crosstab(df_not_processed['cluster_kmeans_3'], df_not_processed['Gender'], normalize='index')\n",
        "gender_dist.plot(kind='bar', stacked=True, ax=axes[1, 2])\n",
        "axes[1, 2].set_xlabel('Кластер')\n",
        "axes[1, 2].set_ylabel('Доля')\n",
        "axes[1, 2].set_title('Распределение по полу в кластерах')\n",
        "axes[1, 2].legend(title='Пол')\n",
        "axes[1, 2].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Анализ кластеров\n",
        "print(\"\\nАНАЛИЗ КЛАСТЕРОВ K-MEANS (3 КЛАСТЕРА):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Для каждого кластера выведем характеристику\n",
        "for cluster_num in range(3):\n",
        "    cluster_data = df_not_processed[df_not_processed['cluster_kmeans_3'] == cluster_num]\n",
        "\n",
        "    print(f\"\\nКЛАСТЕР {cluster_num} ({len(cluster_data)} наблюдений, {len(cluster_data)/len(df_not_processed)*100:.1f}%):\")\n",
        "\n",
        "    # Средние значения ключевых числовых признаков\n",
        "    print(\"  Средние значения ключевых признаков:\")\n",
        "    key_features = ['Age', 'Sleep Duration', 'Physical Activity Level', 'Stress Level', 'Heart Rate']\n",
        "    for feature in key_features:\n",
        "        mean_val = cluster_data[feature].mean()\n",
        "        print(f\"    {feature}: {mean_val:.2f}\")\n",
        "\n",
        "    # Распределение по категориальным признакам\n",
        "    print(\"  Распределение по категориальным признакам:\")\n",
        "\n",
        "    # Пол\n",
        "    gender_dist = cluster_data['Gender'].value_counts(normalize=True)\n",
        "    print(f\"    Пол: {gender_dist.idxmax()} ({gender_dist.max()*100:.1f}%)\")\n",
        "\n",
        "    # Категория BMI\n",
        "    if 'BMI Category' in cluster_data.columns:\n",
        "        bmi_dist = cluster_data['BMI Category'].value_counts(normalize=True)\n",
        "        if not bmi_dist.empty:\n",
        "            print(f\"    Категория BMI: {bmi_dist.idxmax()} ({bmi_dist.max()*100:.1f}%)\")\n",
        "\n",
        "    # Расстройство сна\n",
        "    if 'Sleep Disorder' in cluster_data.columns:\n",
        "        disorder_dist = cluster_data['Sleep Disorder'].value_counts(normalize=True)\n",
        "        if not disorder_dist.empty:\n",
        "            print(f\"    Расстройство сна: {disorder_dist.idxmax()} ({disorder_dist.max()*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "SH7IbqgbfR-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 6: Метод локтя для определения оптимального числа кластеров"
      ],
      "metadata": {
        "id": "r0CPnar5fTig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 6: МЕТОД ЛОКТЯ ДЛЯ ОПРЕДЕЛЕНИЯ ОПТИМАЛЬНОГО ЧИСЛА КЛАСТЕРОВ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Используем метод локтя для определения оптимального числа кластеров\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "calinski_scores = []\n",
        "davies_scores = []\n",
        "\n",
        "# Пробуем разное количество кластеров от 2 до 10\n",
        "k_range = range(2, 11)\n",
        "\n",
        "print(\"Вычисляем метрики для разного количества кластеров...\")\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X, clusters))\n",
        "    calinski_scores.append(calinski_harabasz_score(X, clusters))\n",
        "    davies_scores.append(davies_bouldin_score(X, clusters))\n",
        "\n",
        "    print(f\"  k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.4f}\")\n",
        "\n",
        "# Визуализируем результаты\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Метод локтя (Inertia)\n",
        "axes[0, 0].plot(k_range, inertia, marker='o')\n",
        "axes[0, 0].set_xlabel('Количество кластеров (k)')\n",
        "axes[0, 0].set_ylabel('Inertia (сумма квадратов расстояний)')\n",
        "axes[0, 0].set_title('Метод локтя для K-means')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Добавляем локти (изгибы) для наглядности\n",
        "# Вычисляем разности второго порядка для нахождения \"локтя\"\n",
        "differences = np.diff(inertia)\n",
        "second_diff = np.diff(differences)\n",
        "elbow_point = np.argmin(second_diff) + 2  # +2 потому что мы потеряли два элемента\n",
        "\n",
        "axes[0, 0].axvline(x=elbow_point, color='r', linestyle='--', alpha=0.7,\n",
        "                   label=f'Предполагаемый локоть: k={elbow_point}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 2. Silhouette Score\n",
        "axes[0, 1].plot(k_range, silhouette_scores, marker='o', color='green')\n",
        "axes[0, 1].set_xlabel('Количество кластеров (k)')\n",
        "axes[0, 1].set_ylabel('Silhouette Score')\n",
        "axes[0, 1].set_title('Silhouette Score для разного k')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Находим k с максимальным Silhouette Score\n",
        "best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
        "axes[0, 1].axvline(x=best_k_silhouette, color='r', linestyle='--', alpha=0.7,\n",
        "                   label=f'Лучший k по Silhouette: {best_k_silhouette}')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Calinski-Harabasz Index\n",
        "axes[1, 0].plot(k_range, calinski_scores, marker='o', color='orange')\n",
        "axes[1, 0].set_xlabel('Количество кластеров (k)')\n",
        "axes[1, 0].set_ylabel('Calinski-Harabasz Index')\n",
        "axes[1, 0].set_title('Calinski-Harabasz Index для разного k')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Находим k с максимальным Calinski-Harabasz\n",
        "best_k_calinski = k_range[np.argmax(calinski_scores)]\n",
        "axes[1, 0].axvline(x=best_k_calinski, color='r', linestyle='--', alpha=0.7,\n",
        "                   label=f'Лучший k по Calinski: {best_k_calinski}')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 4. Davies-Bouldin Index\n",
        "axes[1, 1].plot(k_range, davies_scores, marker='o', color='purple')\n",
        "axes[1, 1].set_xlabel('Количество кластеров (k)')\n",
        "axes[1, 1].set_ylabel('Davies-Bouldin Index')\n",
        "axes[1, 1].set_title('Davies-Bouldin Index для разного k')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Находим k с минимальным Davies-Bouldin\n",
        "best_k_davies = k_range[np.argmin(davies_scores)]\n",
        "axes[1, 1].axvline(x=best_k_davies, color='r', linestyle='--', alpha=0.7,\n",
        "                   label=f'Лучший k по Davies: {best_k_davies}')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Анализируем результаты\n",
        "print(\"\\nАНАЛИЗ МЕТОДА ЛОКТЯ:\")\n",
        "print(f\"1. Метод локтя предполагает оптимальное k = {elbow_point}\")\n",
        "print(f\"2. Лучший k по Silhouette Score: {best_k_silhouette}\")\n",
        "print(f\"3. Лучший k по Calinski-Harabasz Index: {best_k_calinski}\")\n",
        "print(f\"4. Лучший k по Davies-Bouldin Index: {best_k_davies}\")\n",
        "\n",
        "# Выбираем оптимальное k на основе всех метрик\n",
        "# Часто выбирают k, где Silhouette Score максимален, а Inertia имеет явный изгиб\n",
        "optimal_k = best_k_silhouette  # Выбираем на основе Silhouette Score\n",
        "print(f\"\\nВыбираем оптимальное k = {optimal_k} (на основе Silhouette Score)\")\n",
        "\n",
        "# Запускаем K-means с оптимальным k\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "clusters_optimal = kmeans_optimal.fit_predict(X)\n",
        "\n",
        "# Добавляем кластеры в датасеты\n",
        "df_not_processed['cluster_kmeans_optimal'] = clusters_optimal\n",
        "df_scaled['cluster_kmeans_optimal'] = clusters_optimal\n",
        "\n",
        "print(f\"\\nK-means с оптимальным k={optimal_k} выполнен\")\n",
        "print(\"Размеры кластеров:\")\n",
        "for i in range(optimal_k):\n",
        "    count = np.sum(clusters_optimal == i)\n",
        "    print(f\"  Кластер {i}: {count} наблюдений ({count/len(clusters_optimal)*100:.1f}%)\")\n",
        "\n",
        "# Вычисляем метрики\n",
        "silhouette_opt = silhouette_score(X, clusters_optimal)\n",
        "calinski_opt = calinski_harabasz_score(X, clusters_optimal)\n",
        "davies_opt = davies_bouldin_score(X, clusters_optimal)\n",
        "\n",
        "print(\"\\nМетрики качества для оптимального k:\")\n",
        "print(f\"  Silhouette Score: {silhouette_opt:.4f}\")\n",
        "print(f\"  Calinski-Harabasz Index: {calinski_opt:.2f}\")\n",
        "print(f\"  Davies-Bouldin Index: {davies_opt:.4f}\")"
      ],
      "metadata": {
        "id": "rlcjJvXXfWGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 7: DBSCAN кластеризация"
      ],
      "metadata": {
        "id": "pjXPUNU4fYi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 7: DBSCAN КЛАСТЕРИЗАЦИЯ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\")\n",
        "print(\"Особенности DBSCAN:\")\n",
        "print(\"- Не требует задания числа кластеров заранее\")\n",
        "print(\"- Может находить кластеры произвольной формы\")\n",
        "print(\"- Помечает выбросы как шум (кластер -1)\")\n",
        "print(\"- Чувствителен к параметрам eps и min_samples\")\n",
        "\n",
        "# Подбираем оптимальные параметры для DBSCAN\n",
        "# Используем метод k-distance графика для выбора eps\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Вычисляем расстояния до k ближайших соседей\n",
        "k = 5  # min_samples обычно выбирают как 2*dim, где dim - размерность данных\n",
        "nn = NearestNeighbors(n_neighbors=k)\n",
        "nn.fit(X)\n",
        "distances, indices = nn.kneighbors(X)\n",
        "\n",
        "# Сортируем расстояния до k-го соседа\n",
        "k_distances = np.sort(distances[:, k-1])\n",
        "\n",
        "# Строим график k-distance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(k_distances)+1), k_distances)\n",
        "plt.xlabel('Точки, отсортированные по расстоянию до k-го соседа')\n",
        "plt.ylabel(f'Расстояние до {k}-го соседа')\n",
        "plt.title('K-distance график для выбора eps')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Ищем \"локоть\" на графике\n",
        "# Вычисляем вторую производную для нахождения точки изгиба\n",
        "gradients = np.gradient(k_distances)\n",
        "second_gradients = np.gradient(gradients)\n",
        "elbow_idx = np.argmax(second_gradients) + 1\n",
        "eps_suggestion = k_distances[elbow_idx]\n",
        "\n",
        "plt.axhline(y=eps_suggestion, color='r', linestyle='--',\n",
        "           label=f'Предлагаемое eps: {eps_suggestion:.3f}')\n",
        "plt.axvline(x=elbow_idx, color='r', linestyle='--', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nНа основе k-distance графика предлагается eps ≈ {eps_suggestion:.3f}\")\n",
        "print(f\"min_samples обычно выбирают как 2 * размерность данных = {2 * X.shape[1]}\")\n",
        "\n",
        "# Пробуем разные параметры\n",
        "param_combinations = [\n",
        "    {'eps': 3.0, 'min_samples': 10},\n",
        "    {'eps': 3.5, 'min_samples': 10},\n",
        "    {'eps': 4.0, 'min_samples': 10},\n",
        "    {'eps': 4.0, 'min_samples': 15},\n",
        "    {'eps': 4.5, 'min_samples': 15},\n",
        "    {'eps': 5.0, 'min_samples': 15},\n",
        "]\n",
        "\n",
        "print(\"\\nПробуем разные комбинации параметров DBSCAN:\")\n",
        "\n",
        "best_score = -1\n",
        "best_params = None\n",
        "best_clusters = None\n",
        "\n",
        "results = []\n",
        "\n",
        "for params in param_combinations:\n",
        "    dbscan = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
        "    clusters = dbscan.fit_predict(X)\n",
        "\n",
        "    # Количество кластеров (исключая шум -1)\n",
        "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
        "\n",
        "    # Процент шума\n",
        "    noise_ratio = np.sum(clusters == -1) / len(clusters) * 100\n",
        "\n",
        "    # Вычисляем метрики только если есть хотя бы 2 кластера и не все точки - шум\n",
        "    if n_clusters >= 2 and noise_ratio < 100:\n",
        "        # Для DBSCAN иногда сложно вычислить метрики из-за шума\n",
        "        # Берем только точки, которые не являются шумом\n",
        "        non_noise_mask = clusters != -1\n",
        "        if np.sum(non_noise_mask) > 1:  # Нужно хотя бы 2 точки не-шума\n",
        "            silhouette = silhouette_score(X[non_noise_mask], clusters[non_noise_mask])\n",
        "            calinski = calinski_harabasz_score(X[non_noise_mask], clusters[non_noise_mask])\n",
        "            davies = davies_bouldin_score(X[non_noise_mask], clusters[non_noise_mask])\n",
        "        else:\n",
        "            silhouette = calinski = davies = np.nan\n",
        "    else:\n",
        "        silhouette = calinski = davies = np.nan\n",
        "\n",
        "    results.append({\n",
        "        'eps': params['eps'],\n",
        "        'min_samples': params['min_samples'],\n",
        "        'n_clusters': n_clusters,\n",
        "        'noise_ratio': noise_ratio,\n",
        "        'silhouette': silhouette,\n",
        "        'calinski': calinski,\n",
        "        'davies': davies\n",
        "    })\n",
        "\n",
        "    print(f\"  eps={params['eps']}, min_samples={params['min_samples']}: \"\n",
        "          f\"{n_clusters} кластеров, шум: {noise_ratio:.1f}%\")\n",
        "\n",
        "# Создаем DataFrame с результатами\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\nРезультаты подбора параметров DBSCAN:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Выбираем лучшие параметры на основе Silhouette Score\n",
        "# Игнорируем варианты с NaN\n",
        "valid_results = results_df[~results_df['silhouette'].isna()]\n",
        "if not valid_results.empty:\n",
        "    best_idx = valid_results['silhouette'].idxmax()\n",
        "    best_params = {\n",
        "        'eps': valid_results.loc[best_idx, 'eps'],\n",
        "        'min_samples': valid_results.loc[best_idx, 'min_samples']\n",
        "    }\n",
        "else:\n",
        "    # Если все результаты с NaN, берем с наименьшим процентом шума и хотя бы 2 кластера\n",
        "    valid_results = results_df[results_df['n_clusters'] >= 2]\n",
        "    if not valid_results.empty:\n",
        "        best_idx = valid_results['noise_ratio'].idxmin()\n",
        "        best_params = {\n",
        "            'eps': valid_results.loc[best_idx, 'eps'],\n",
        "            'min_samples': valid_results.loc[best_idx, 'min_samples']\n",
        "        }\n",
        "    else:\n",
        "        # Если ничего не подошло, берем первый вариант\n",
        "        best_params = param_combinations[0]\n",
        "\n",
        "print(f\"\\nВыбранные параметры: eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\n",
        "\n",
        "# Запускаем DBSCAN с выбранными параметрами\n",
        "dbscan_best = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "clusters_dbscan = dbscan_best.fit_predict(X)\n",
        "\n",
        "# Добавляем кластеры в датасеты\n",
        "df_not_processed['cluster_dbscan'] = clusters_dbscan\n",
        "df_scaled['cluster_dbscan'] = clusters_dbscan\n",
        "\n",
        "# Анализируем результаты\n",
        "unique_clusters = np.unique(clusters_dbscan)\n",
        "n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
        "noise_count = np.sum(clusters_dbscan == -1)\n",
        "noise_ratio = noise_count / len(clusters_dbscan) * 100\n",
        "\n",
        "print(f\"\\nDBSCAN кластеризация завершена:\")\n",
        "print(f\"  Всего уникальных меток: {unique_clusters}\")\n",
        "print(f\"  Количество кластеров (без шума): {n_clusters}\")\n",
        "print(f\"  Количество точек шума: {noise_count} ({noise_ratio:.1f}%)\")\n",
        "\n",
        "if n_clusters >= 2:\n",
        "    # Вычисляем метрики для не-шумовых точек\n",
        "    non_noise_mask = clusters_dbscan != -1\n",
        "    if np.sum(non_noise_mask) > 1:\n",
        "        silhouette_db = silhouette_score(X[non_noise_mask], clusters_dbscan[non_noise_mask])\n",
        "        calinski_db = calinski_harabasz_score(X[non_noise_mask], clusters_dbscan[non_noise_mask])\n",
        "        davies_db = davies_bouldin_score(X[non_noise_mask], clusters_dbscan[non_noise_mask])\n",
        "\n",
        "        print(f\"\\nМетрики качества DBSCAN (без шума):\")\n",
        "        print(f\"  Silhouette Score: {silhouette_db:.4f}\")\n",
        "        print(f\"  Calinski-Harabasz Index: {calinski_db:.2f}\")\n",
        "        print(f\"  Davies-Bouldin Index: {davies_db:.4f}\")\n",
        "    else:\n",
        "        print(\"Недостаточно не-шумовых точек для вычисления метрик\")\n",
        "else:\n",
        "    print(\"DBSCAN нашел менее 2 кластеров, метрики не вычисляются\")"
      ],
      "metadata": {
        "id": "irvEtYiifclZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 8: Визуализация и анализ DBSCAN кластеризации"
      ],
      "metadata": {
        "id": "vs-ooZOofhbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 8: ВИЗУАЛИЗАЦИЯ И АНАЛИЗ DBSCAN КЛАСТЕРИЗАЦИИ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Визуализируем результаты DBSCAN\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. DBSCAN в пространстве PCA (все точки)\n",
        "scatter_dbscan = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_dbscan,\n",
        "                                    cmap='tab20', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('PCA Component 1')\n",
        "axes[0, 0].set_ylabel('PCA Component 2')\n",
        "axes[0, 0].set_title(f'DBSCAN кластеризация (eps={best_params[\"eps\"]}, min_samples={best_params[\"min_samples\"]})')\n",
        "plt.colorbar(scatter_dbscan, ax=axes[0, 0])\n",
        "\n",
        "# 2. DBSCAN в пространстве PCA (шум отдельно)\n",
        "# Создаем специальную цветовую схему: шум - серый, кластеры - разные цвета\n",
        "dbscan_colors = []\n",
        "color_map = plt.cm.tab20\n",
        "for cluster in clusters_dbscan:\n",
        "    if cluster == -1:\n",
        "        dbscan_colors.append((0.5, 0.5, 0.5, 0.5))  # Серый для шума\n",
        "    else:\n",
        "        dbscan_colors.append(color_map(cluster % 20))\n",
        "\n",
        "axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_colors, alpha=0.7)\n",
        "axes[0, 1].set_xlabel('PCA Component 1')\n",
        "axes[0, 1].set_ylabel('PCA Component 2')\n",
        "axes[0, 1].set_title('DBSCAN (шум серым)')\n",
        "\n",
        "# 3. Распределение кластеров DBSCAN\n",
        "dbscan_counts = df_not_processed['cluster_dbscan'].value_counts().sort_index()\n",
        "bars_dbscan = axes[0, 2].bar(dbscan_counts.index, dbscan_counts.values)\n",
        "axes[0, 2].set_xlabel('Кластер ( -1 = шум )')\n",
        "axes[0, 2].set_ylabel('Количество наблюдений')\n",
        "axes[0, 2].set_title('Распределение по кластерам DBSCAN')\n",
        "\n",
        "# Подписываем количество на каждом столбце\n",
        "for bar in bars_dbscan:\n",
        "    height = bar.get_height()\n",
        "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                   f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Сравнение K-means и DBSCAN\n",
        "# Выделяем шум от DBSCAN на K-means кластеризации\n",
        "noise_mask = clusters_dbscan == -1\n",
        "axes[1, 0].scatter(X_pca[~noise_mask, 0], X_pca[~noise_mask, 1],\n",
        "                   c=clusters_optimal[~noise_mask], cmap='viridis', alpha=0.7, label='K-means кластеры')\n",
        "axes[1, 0].scatter(X_pca[noise_mask, 0], X_pca[noise_mask, 1],\n",
        "                   c='red', marker='x', s=50, label='DBSCAN шум')\n",
        "axes[1, 0].set_xlabel('PCA Component 1')\n",
        "axes[1, 0].set_ylabel('PCA Component 2')\n",
        "axes[1, 0].set_title('K-means кластеры с DBSCAN шумом (красные X)')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 5. DBSCAN в пространстве t-SNE\n",
        "scatter_dbscan_tsne = axes[1, 1].scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
        "                                         c=clusters_dbscan, cmap='tab20', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('t-SNE Component 1')\n",
        "axes[1, 1].set_ylabel('t-SNE Component 2')\n",
        "axes[1, 1].set_title('DBSCAN - t-SNE проекция')\n",
        "\n",
        "# 6. Анализ характеристик шумовых точек\n",
        "if noise_count > 0:\n",
        "    noise_data = df_not_processed[df_not_processed['cluster_dbscan'] == -1]\n",
        "    non_noise_data = df_not_processed[df_not_processed['cluster_dbscan'] != -1]\n",
        "\n",
        "    # Сравним средние значения ключевых признаков\n",
        "    key_features = ['Age', 'Sleep Duration', 'Physical Activity Level', 'Stress Level']\n",
        "\n",
        "    noise_means = noise_data[key_features].mean()\n",
        "    non_noise_means = non_noise_data[key_features].mean()\n",
        "\n",
        "    x = np.arange(len(key_features))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[1, 2].bar(x - width/2, noise_means, width, label='Шумовые точки', alpha=0.7)\n",
        "    axes[1, 2].bar(x + width/2, non_noise_means, width, label='Кластеризованные точки', alpha=0.7)\n",
        "\n",
        "    axes[1, 2].set_xlabel('Признаки')\n",
        "    axes[1, 2].set_ylabel('Среднее значение')\n",
        "    axes[1, 2].set_title('Сравнение шумовых и кластеризованных точек')\n",
        "    axes[1, 2].set_xticks(x)\n",
        "    axes[1, 2].set_xticklabels(key_features, rotation=45, ha='right')\n",
        "    axes[1, 2].legend()\n",
        "else:\n",
        "    axes[1, 2].text(0.5, 0.5, 'Нет шумовых точек\\nв DBSCAN кластеризации',\n",
        "                    ha='center', va='center', fontsize=12)\n",
        "    axes[1, 2].set_title('Анализ шумовых точек')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Анализ DBSCAN кластеров\n",
        "print(\"\\nАНАЛИЗ DBSCAN КЛАСТЕРОВ:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if n_clusters > 0:\n",
        "    for cluster_num in sorted([c for c in unique_clusters if c != -1]):\n",
        "        cluster_data = df_not_processed[df_not_processed['cluster_dbscan'] == cluster_num]\n",
        "\n",
        "        print(f\"\\nКЛАСТЕР {cluster_num} ({len(cluster_data)} наблюдений, {len(cluster_data)/len(df_not_processed)*100:.1f}%):\")\n",
        "\n",
        "        # Средние значения ключевых числовых признаков\n",
        "        print(\"  Средние значения ключевых признаков:\")\n",
        "        key_features = ['Age', 'Sleep Duration', 'Physical Activity Level', 'Stress Level']\n",
        "        for feature in key_features:\n",
        "            mean_val = cluster_data[feature].mean()\n",
        "            print(f\"    {feature}: {mean_val:.2f}\")\n",
        "\n",
        "        # Распределение по полу\n",
        "        if len(cluster_data) > 0:\n",
        "            gender_dist = cluster_data['Gender'].value_counts(normalize=True)\n",
        "            if not gender_dist.empty:\n",
        "                print(f\"    Пол: {gender_dist.idxmax()} ({gender_dist.max()*100:.1f}%)\")\n",
        "else:\n",
        "    print(\"DBSCAN не обнаружил кластеров (только шум)\")\n",
        "\n",
        "# Анализ шумовых точек\n",
        "if noise_count > 0:\n",
        "    print(f\"\\nАНАЛИЗ ШУМОВЫХ ТОЧЕК ({noise_count} точек, {noise_ratio:.1f}%):\")\n",
        "    noise_data = df_not_processed[df_not_processed['cluster_dbscan'] == -1]\n",
        "\n",
        "    print(\"  Средние значения ключевых признаков:\")\n",
        "    for feature in key_features:\n",
        "        mean_val = noise_data[feature].mean()\n",
        "        print(f\"    {feature}: {mean_val:.2f}\")\n",
        "\n",
        "    # Проверяем, в какие K-means кластеры попадают шумовые точки\n",
        "    print(f\"  Распределение по K-means кластерам (k={optimal_k}):\")\n",
        "    kmeans_dist = noise_data['cluster_kmeans_optimal'].value_counts()\n",
        "    for k_cluster, count in kmeans_dist.items():\n",
        "        print(f\"    K-means кластер {k_cluster}: {count} точек ({count/len(noise_data)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "wA-pgtrOfnCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 9: Сравнение методов кластеризации"
      ],
      "metadata": {
        "id": "Nl0fK5-KfpGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТЬ 9: СРАВНЕНИЕ МЕТОДОВ КЛАСТЕРИЗАЦИИ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Сравниваем K-means и DBSCAN\n",
        "comparison_data = []\n",
        "\n",
        "# 1. K-means с 3 кластерами\n",
        "comparison_data.append({\n",
        "    'Метод': 'K-means (k=3)',\n",
        "    'Количество кластеров': 3,\n",
        "    'Процент шума': 0.0,\n",
        "    'Silhouette Score': silhouette_3,\n",
        "    'Calinski-Harabasz': calinski_3,\n",
        "    'Davies-Bouldin': davies_3\n",
        "})\n",
        "\n",
        "# 2. K-means с оптимальным k\n",
        "comparison_data.append({\n",
        "    'Метод': f'K-means (оптимальное k={optimal_k})',\n",
        "    'Количество кластеров': optimal_k,\n",
        "    'Процент шума': 0.0,\n",
        "    'Silhouette Score': silhouette_opt,\n",
        "    'Calinski-Harabasz': calinski_opt,\n",
        "    'Davies-Bouldin': davies_opt\n",
        "})\n",
        "\n",
        "# 3. DBSCAN\n",
        "if n_clusters >= 2 and 'silhouette_db' in locals():\n",
        "    comparison_data.append({\n",
        "        'Метод': f'DBSCAN (eps={best_params[\"eps\"]}, min_samples={best_params[\"min_samples\"]})',\n",
        "        'Количество кластеров': n_clusters,\n",
        "        'Процент шума': noise_ratio,\n",
        "        'Silhouette Score': silhouette_db,\n",
        "        'Calinski-Harabasz': calinski_db,\n",
        "        'Davies-Bouldin': davies_db\n",
        "    })\n",
        "\n",
        "# Создаем DataFrame для сравнения\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nСРАВНЕНИЕ МЕТОДОВ КЛАСТЕРИЗАЦИИ:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Визуализируем сравнение\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Сравнение количества кластеров\n",
        "methods = comparison_df['Метод'].tolist()\n",
        "n_clusters_list = comparison_df['Количество кластеров'].tolist()\n",
        "\n",
        "bars1 = axes[0, 0].bar(methods, n_clusters_list)\n",
        "axes[0, 0].set_ylabel('Количество кластеров')\n",
        "axes[0, 0].set_title('Сравнение количества кластеров')\n",
        "axes[0, 0].tick_params(axis='x', rotation=15)\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                   f'{int(height)}', ha='center', va='bottom')\n",
        "\n",
        "# 2. Сравнение Silhouette Score\n",
        "silhouette_list = comparison_df['Silhouette Score'].tolist()\n",
        "\n",
        "bars2 = axes[0, 1].bar(methods, silhouette_list, color='green')\n",
        "axes[0, 1].set_ylabel('Silhouette Score')\n",
        "axes[0, 1].set_title('Сравнение Silhouette Score (чем выше, тем лучше)')\n",
        "axes[0, 1].tick_params(axis='x', rotation=15)\n",
        "axes[0, 1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Хорошая кластеризация')\n",
        "axes[0, 1].legend()\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                   f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 3. Сравнение Calinski-Harabasz Index\n",
        "calinski_list = comparison_df['Calinski-Harabasz'].tolist()\n",
        "\n",
        "bars3 = axes[1, 0].bar(methods, calinski_list, color='orange')\n",
        "axes[1, 0].set_ylabel('Calinski-Harabasz Index')\n",
        "axes[1, 0].set_title('Сравнение Calinski-Harabasz (чем выше, тем лучше)')\n",
        "axes[1, 0].tick_params(axis='x', rotation=15)\n",
        "for bar in bars3:\n",
        "    height = bar.get_height()\n",
        "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                   f'{height:.0f}', ha='center', va='bottom')\n",
        "\n",
        "# 4. Сравнение Davies-Bouldin Index\n",
        "davies_list = comparison_df['Davies-Bouldin'].tolist()\n",
        "\n",
        "bars4 = axes[1, 1].bar(methods, davies_list, color='purple')\n",
        "axes[1, 1].set_ylabel('Davies-Bouldin Index')\n",
        "axes[1, 1].set_title('Сравнение Davies-Bouldin (чем ниже, тем лучше)')\n",
        "axes[1, 1].tick_params(axis='x', rotation=15)\n",
        "for bar in bars4:\n",
        "    height = bar.get_height()\n",
        "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                   f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Итоговый анализ и выводы\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ИТОГОВЫЙ АНАЛИЗ И ВЫВОДЫ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. АНАЛИЗ K-MEANS:\")\n",
        "print(\"   - Метод локтя показал оптимальное k =\", elbow_point)\n",
        "print(f\"   - Silhouette Score максимизируется при k = {best_k_silhouette}\")\n",
        "print(f\"   - K-means создает четкие, сферические кластеры одинакового размера\")\n",
        "print(f\"   - Не учитывает шумовые точки и выбросы\")\n",
        "\n",
        "print(\"\\n2. АНАЛИЗ DBSCAN:\")\n",
        "print(f\"   - Обнаружил {n_clusters} кластеров и {noise_ratio:.1f}% шума\")\n",
        "print(f\"   - Может находить кластеры произвольной формы\")\n",
        "print(f\"   - Автоматически идентифицирует выбросы как шум\")\n",
        "print(f\"   - Чувствителен к параметрам eps и min_samples\")\n",
        "\n",
        "print(\"\\n3. СРАВНЕНИЕ МЕТОДОВ:\")\n",
        "print(\"   - K-means лучше подходит для данных со сферическими кластерами\")\n",
        "print(\"   - DBSCAN лучше для данных с кластерами произвольной формы и наличием выбросов\")\n",
        "print(\"   - K-means требует задания числа кластеров заранее\")\n",
        "print(\"   - DBSCAN определяет число кластеров автоматически\")\n",
        "\n",
        "print(\"\\n4. РЕКОМЕНДАЦИИ ДЛЯ ДАННОГО ДАТАСЕТА:\")\n",
        "\n",
        "# На основе метрик и визуального анализа делаем вывод\n",
        "if 'silhouette_db' in locals():\n",
        "    if silhouette_opt > silhouette_db and silhouette_opt > 0.5:\n",
        "        print(\"   - K-means показывает лучшие результаты на данном датасете\")\n",
        "        print(f\"   - Silhouette Score K-means ({silhouette_opt:.3f}) > DBSCAN ({silhouette_db:.3f})\")\n",
        "        print(\"   - Рекомендуется использовать K-means с оптимальным k\")\n",
        "    elif silhouette_db > silhouette_opt and silhouette_db > 0.5:\n",
        "        print(\"   - DBSCAN показывает лучшие результаты на данном датасете\")\n",
        "        print(f\"   - Silhouette Score DBSCAN ({silhouette_db:.3f}) > K-means ({silhouette_opt:.3f})\")\n",
        "        print(\"   - Рекомендуется использовать DBSCAN\")\n",
        "    else:\n",
        "        print(\"   - Оба метода показывают средние результаты\")\n",
        "        print(\"   - Возможно, данные плохо кластеризуются или нужны другие методы\")\n",
        "else:\n",
        "    print(\"   - K-means показал лучшие результаты (DBSCAN не нашел кластеров или метрики не вычислены)\")\n",
        "    print(f\"   - Silhouette Score K-means: {silhouette_opt:.3f}\")\n",
        "    print(\"   - Рекомендуется использовать K-means\")\n",
        "\n",
        "print(\"\\n5. ПРАКТИЧЕСКИЕ ВЫВОДЫ ДЛЯ АНАЛИЗА СНА:\")\n",
        "print(\"   - На основе кластеризации можно выделить группы пациентов с похожими характеристиками\")\n",
        "print(\"   - K-means с оптимальным k позволяет сегментировать пациентов на группы\")\n",
        "print(\"   - Эти группы могут помочь в персонализированных рекомендациях по улучшению сна\")\n",
        "print(\"   - Выявленные кластеры могут соответствовать разным типам расстройств сна\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЗАКЛЮЧЕНИЕ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nВ рамках домашнего задания выполнено:\")\n",
        "print(\"1. ✓ Предобработка данных: One-Hot Encoding категориальных признаков, масштабирование\")\n",
        "print(\"2. ✓ K-means кластеризация с анализом результатов\")\n",
        "print(\"3. ✓ Определение оптимального числа кластеров методом локтя\")\n",
        "print(\"4. ✓ DBSCAN кластеризация с подбором параметров\")\n",
        "print(\"5. ✓ Сравнение методов и выбор наиболее подходящего\")\n",
        "\n",
        "print(f\"\\nНаиболее подходящий метод для данного датасета: {'K-means' if 'silhouette_db' not in locals() or silhouette_opt >= silhouette_db else 'DBSCAN'}\")\n",
        "print(\"\\nРезультаты кластеризации сохранены в df_not_processed с колонками:\")\n",
        "print(\"  - cluster_kmeans_3: K-means с 3 кластерами\")\n",
        "print(f\"  - cluster_kmeans_optimal: K-means с оптимальным k={optimal_k}\")\n",
        "print(\"  - cluster_dbscan: DBSCAN кластеризация\")"
      ],
      "metadata": {
        "id": "aXGzs3xSfr2I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}