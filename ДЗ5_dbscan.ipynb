{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orutkina/-./blob/main/%D0%94%D0%975_dbscan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание 5.2\n",
        "\n",
        "Структура проекта:\n"
      ],
      "metadata": {
        "id": "Q2GrfXXruCXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_parser/\n",
        "│\n",
        "├── parser/\n",
        "│   ├── __init__.py\n",
        "│   ├── base_parser.py\n",
        "│   ├── ria_parser.py\n",
        "│   ├── lenta_parser.py\n",
        "│   ├── gazeta_parser.py\n",
        "│   └── kommersant_parser.py\n",
        "│\n",
        "├── database/\n",
        "│   ├── __init__.py\n",
        "│   └── db_manager.py\n",
        "│\n",
        "├── utils/\n",
        "│   ├── __init__.py\n",
        "│   ├── text_cleaner.py\n",
        "│   ├── rate_limiter.py\n",
        "│   └── user_agents.py\n",
        "│\n",
        "├── config.py\n",
        "├── main.py\n",
        "├── requirements.txt\n",
        "└── README.md"
      ],
      "metadata": {
        "id": "tQuJWR6TlNVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Файл requirements.txt:"
      ],
      "metadata": {
        "id": "FrviTBM7lPhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "requests>=2.28.0\n",
        "beautifulsoup4>=4.11.0\n",
        "lxml>=4.9.0\n",
        "sqlite3\n",
        "uuid\n",
        "python-dateutil>=2.8.0\n",
        "aiohttp>=3.8.0\n",
        "asyncio\n",
        "pandas>=1.5.0"
      ],
      "metadata": {
        "id": "wcntXxEWlS1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Файл config.py:"
      ],
      "metadata": {
        "id": "rnJzZq3klcU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Настройки базы данных\n",
        "DB_NAME = \"news_articles.db\"\n",
        "DB_PATH = os.path.join(os.path.dirname(__file__), DB_NAME)\n",
        "\n",
        "# Настройки парсинга\n",
        "MAX_REQUESTS_PER_SECOND = 2\n",
        "REQUEST_TIMEOUT = 30\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Список сайтов для парсинга\n",
        "NEWS_SITES = [\n",
        "    {\n",
        "        \"name\": \"ria\",\n",
        "        \"base_url\": \"https://ria.ru\",\n",
        "        \"rss_url\": \"https://ria.ru/export/rss2/archive/index.xml\",\n",
        "        \"parser_class\": \"RiaParser\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"lenta\",\n",
        "        \"base_url\": \"https://lenta.ru\",\n",
        "        \"rss_url\": \"https://lenta.ru/rss\",\n",
        "        \"parser_class\": \"LentaParser\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"gazeta\",\n",
        "        \"base_url\": \"https://www.gazeta.ru\",\n",
        "        \"rss_url\": \"https://www.gazeta.ru/export/rss/lenta.xml\",\n",
        "        \"parser_class\": \"GazetaParser\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"kommersant\",\n",
        "        \"base_url\": \"https://www.kommersant.ru\",\n",
        "        \"rss_url\": \"https://www.kommersant.ru/RSS/news.xml\",\n",
        "        \"parser_class\": \"KommersantParser\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"tass\",\n",
        "        \"base_url\": \"https://tass.ru\",\n",
        "        \"rss_url\": \"https://tass.ru/rss/v2.xml\",\n",
        "        \"parser_class\": \"TassParser\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"rbc\",\n",
        "        \"base_url\": \"https://www.rbc.ru\",\n",
        "        \"rss_url\": \"https://rssexport.rbc.ru/rbcnews/news/30/full.rss\",\n",
        "        \"parser_class\": \"RbcParser\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Минимальное количество статей\n",
        "MIN_ARTICLES_COUNT = 5000\n",
        "\n",
        "# Настройки очистки текста\n",
        "MIN_ARTICLE_LENGTH = 100  # минимальная длина статьи в символах"
      ],
      "metadata": {
        "id": "rYTjTfXxle7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Файл database/db_manager.py:"
      ],
      "metadata": {
        "id": "XBcrnhrglguK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict, Any\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DatabaseManager:\n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.create_tables()\n",
        "\n",
        "    def create_tables(self):\n",
        "        \"\"\"Создание таблиц в базе данных\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            # Таблица статей\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS articles (\n",
        "                    guid TEXT PRIMARY KEY,\n",
        "                    title TEXT NOT NULL,\n",
        "                    description TEXT NOT NULL,\n",
        "                    url TEXT UNIQUE NOT NULL,\n",
        "                    published_at TEXT,\n",
        "                    comments_count INTEGER DEFAULT 0,\n",
        "                    created_at_utc TEXT NOT NULL,\n",
        "                    rating REAL,\n",
        "                    source TEXT NOT NULL,\n",
        "                    word_count INTEGER DEFAULT 0,\n",
        "                    category TEXT\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Индексы для оптимизации запросов\n",
        "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON articles(url)')\n",
        "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON articles(source)')\n",
        "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_published ON articles(published_at)')\n",
        "\n",
        "            conn.commit()\n",
        "\n",
        "    def insert_article(self, article_data: Dict[str, Any]) -> bool:\n",
        "        \"\"\"\n",
        "        Вставка статьи в базу данных\n",
        "\n",
        "        Args:\n",
        "            article_data: Словарь с данными статьи\n",
        "\n",
        "        Returns:\n",
        "            bool: Успешность операции\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Генерация GUID если не предоставлен\n",
        "            if 'guid' not in article_data:\n",
        "                article_data['guid'] = str(uuid.uuid4())\n",
        "\n",
        "            # Добавление времени создания\n",
        "            if 'created_at_utc' not in article_data:\n",
        "                article_data['created_at_utc'] = datetime.utcnow().isoformat()\n",
        "\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "\n",
        "                cursor.execute('''\n",
        "                    INSERT OR IGNORE INTO articles\n",
        "                    (guid, title, description, url, published_at, comments_count,\n",
        "                     created_at_utc, rating, source, word_count, category)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', (\n",
        "                    article_data['guid'],\n",
        "                    article_data['title'],\n",
        "                    article_data['description'],\n",
        "                    article_data['url'],\n",
        "                    article_data.get('published_at'),\n",
        "                    article_data.get('comments_count', 0),\n",
        "                    article_data['created_at_utc'],\n",
        "                    article_data.get('rating'),\n",
        "                    article_data.get('source', 'unknown'),\n",
        "                    article_data.get('word_count', 0),\n",
        "                    article_data.get('category')\n",
        "                ))\n",
        "\n",
        "                conn.commit()\n",
        "                return cursor.rowcount > 0\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            logger.error(f\"Ошибка при вставке статьи: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_article_count(self) -> int:\n",
        "        \"\"\"Получение количества статей в базе\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('SELECT COUNT(*) FROM articles')\n",
        "            return cursor.fetchone()[0]\n",
        "\n",
        "    def get_articles_by_source(self, source: str, limit: int = 100):\n",
        "        \"\"\"Получение статей по источнику\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            conn.row_factory = sqlite3.Row\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\n",
        "                'SELECT * FROM articles WHERE source = ? LIMIT ?',\n",
        "                (source, limit)\n",
        "            )\n",
        "            return [dict(row) for row in cursor.fetchall()]\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Закрытие соединения с базой данных\"\"\"\n",
        "        pass  # SQLite автоматически закрывает соединение"
      ],
      "metadata": {
        "id": "7ZUFKl2Mljfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Файл utils/rate_limiter.py:"
      ],
      "metadata": {
        "id": "fPueIhD1llkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "from typing import List\n",
        "import random\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, max_requests_per_second: int = 2):\n",
        "        self.max_requests_per_second = max_requests_per_second\n",
        "        self.min_delay = 1.0 / max_requests_per_second\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    async def wait(self):\n",
        "        \"\"\"Ожидание перед следующим запросом\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "\n",
        "        if time_since_last < self.min_delay:\n",
        "            wait_time = self.min_delay - time_since_last\n",
        "            # Добавляем небольшую случайную задержку\n",
        "            wait_time += random.uniform(0, 0.5)\n",
        "            await asyncio.sleep(wait_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "    def sync_wait(self):\n",
        "        \"\"\"Синхронная версия ожидания\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "\n",
        "        if time_since_last < self.min_delay:\n",
        "            wait_time = self.min_delay - time_since_last\n",
        "            wait_time += random.uniform(0, 0.5)\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "class RequestRetry:\n",
        "    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n",
        "        self.max_retries = max_retries\n",
        "        self.base_delay = base_delay\n",
        "\n",
        "    async def execute(self, func, *args, **kwargs):\n",
        "        \"\"\"Выполнение функции с повторными попытками\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                return await func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    raise e\n",
        "\n",
        "                delay = self.base_delay * (2 ** attempt)  # Экспоненциальная задержка\n",
        "                delay += random.uniform(0, 0.5)  # Случайное добавление\n",
        "                logger.warning(f\"Попытка {attempt + 1} не удалась. Повтор через {delay:.2f} сек. Ошибка: {e}\")\n",
        "                await asyncio.sleep(delay)\n",
        "\n",
        "    def sync_execute(self, func, *args, **kwargs):\n",
        "        \"\"\"Синхронная версия с повторными попытками\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    raise e\n",
        "\n",
        "                delay = self.base_delay * (2 ** attempt)\n",
        "                delay += random.uniform(0, 0.5)\n",
        "                logger.warning(f\"Попытка {attempt + 1} не удалась. Повтор через {delay:.2f} сек. Ошибка: {e}\")\n",
        "                time.sleep(delay)"
      ],
      "metadata": {
        "id": "buElm_7Vlohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Файл utils/user_agents.py:"
      ],
      "metadata": {
        "id": "P0QiMxgKlsZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USER_AGENTS = [\n",
        "    # Chrome\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "\n",
        "    # Firefox\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',\n",
        "\n",
        "    # Safari\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n",
        "\n",
        "    # Edge\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',\n",
        "\n",
        "    # Opera\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0',\n",
        "\n",
        "    # Mobile\n",
        "    'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',\n",
        "    'Mozilla/5.0 (Linux; Android 14; SM-S901B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.210 Mobile Safari/537.36',\n",
        "]\n",
        "\n",
        "def get_random_user_agent():\n",
        "    \"\"\"Получение случайного User-Agent\"\"\"\n",
        "    import random\n",
        "    return random.choice(USER_AGENTS)"
      ],
      "metadata": {
        "id": "fwTx1X9TlzZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Файл utils/text_cleaner.py:"
      ],
      "metadata": {
        "id": "1aCdgKE9l1OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional\n",
        "\n",
        "class TextCleaner:\n",
        "    @staticmethod\n",
        "    def clean_html(html_content: str) -> str:\n",
        "        \"\"\"\n",
        "        Очистка HTML от тегов и медиа-контента\n",
        "\n",
        "        Args:\n",
        "            html_content: Исходный HTML\n",
        "\n",
        "        Returns:\n",
        "            Очищенный текст\n",
        "        \"\"\"\n",
        "        if not html_content:\n",
        "            return \"\"\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "        # Удаление ненужных элементов\n",
        "        for element in soup(['script', 'style', 'iframe', 'object', 'embed',\n",
        "                            'video', 'audio', 'figure', 'img', 'form', 'nav',\n",
        "                            'header', 'footer', 'aside']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Удаление пустых тегов\n",
        "        for tag in soup.find_all():\n",
        "            if len(tag.get_text(strip=True)) == 0:\n",
        "                tag.decompose()\n",
        "\n",
        "        # Получение текста\n",
        "        text = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        # Очистка лишних пробелов и переносов строк\n",
        "        lines = []\n",
        "        for line in text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if line:  # Пропускаем пустые строки\n",
        "                lines.append(line)\n",
        "\n",
        "        # Объединение строк\n",
        "        cleaned_text = '\\n'.join(lines)\n",
        "\n",
        "        # Удаление повторяющихся переносов строк\n",
        "        cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', cleaned_text)\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "    @staticmethod\n",
        "    def count_words(text: str) -> int:\n",
        "        \"\"\"Подсчет слов в тексте\"\"\"\n",
        "        if not text:\n",
        "            return 0\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        return len(words)\n",
        "\n",
        "    @staticmethod\n",
        "    def is_valid_article(text: str, min_length: int = 100) -> bool:\n",
        "        \"\"\"\n",
        "        Проверка валидности статьи\n",
        "\n",
        "        Args:\n",
        "            text: Текст статьи\n",
        "            min_length: Минимальная длина в символах\n",
        "\n",
        "        Returns:\n",
        "            bool: Валидна ли статья\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        # Проверка длины\n",
        "        if len(text.strip()) < min_length:\n",
        "            return False\n",
        "\n",
        "        # Проверка на наличие текста (а не только цифр или спецсимволов)\n",
        "        words = re.findall(r'\\b[а-яА-ЯёЁa-zA-Z]+\\b', text)\n",
        "        if len(words) < 20:  # Минимум 20 слов\n",
        "            return False\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "id": "iAfHDdVEl45C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Файл parser/base_parser.py:"
      ],
      "metadata": {
        "id": "Xfw3c_Cll8BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from utils.rate_limiter import RateLimiter, RequestRetry\n",
        "from utils.user_agents import get_random_user_agent\n",
        "from utils.text_cleaner import TextCleaner\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BaseNewsParser:\n",
        "    def __init__(self, source_name: str, base_url: str):\n",
        "        self.source_name = source_name\n",
        "        self.base_url = base_url\n",
        "        self.rate_limiter = RateLimiter(max_requests_per_second=2)\n",
        "        self.retry_handler = RequestRetry(max_retries=3)\n",
        "        self.text_cleaner = TextCleaner()\n",
        "\n",
        "        # Сессия для асинхронных запросов\n",
        "        self.session = None\n",
        "\n",
        "    async def create_session(self):\n",
        "        \"\"\"Создание асинхронной сессии\"\"\"\n",
        "        if self.session is None:\n",
        "            self.session = aiohttp.ClientSession(\n",
        "                headers={'User-Agent': get_random_user_agent()},\n",
        "                timeout=aiohttp.ClientTimeout(total=30)\n",
        "            )\n",
        "\n",
        "    async def close_session(self):\n",
        "        \"\"\"Закрытие сессии\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def fetch_url(self, url: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Загрузка URL с учетом ограничений\n",
        "\n",
        "        Args:\n",
        "            url: URL для загрузки\n",
        "\n",
        "        Returns:\n",
        "            str: HTML контент или None при ошибке\n",
        "        \"\"\"\n",
        "        await self.rate_limiter.wait()\n",
        "\n",
        "        try:\n",
        "            await self.create_session()\n",
        "\n",
        "            async def _fetch():\n",
        "                async with self.session.get(url) as response:\n",
        "                    if response.status == 200:\n",
        "                        return await response.text()\n",
        "                    else:\n",
        "                        raise Exception(f\"HTTP {response.status}: {response.reason}\")\n",
        "\n",
        "            return await self.retry_handler.execute(_fetch)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при загрузке {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def parse_rss(self, rss_content: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Парсинг RSS для получения ссылок на статьи\n",
        "\n",
        "        Args:\n",
        "            rss_content: XML контент RSS\n",
        "\n",
        "        Returns:\n",
        "            List[str]: Список URL статей\n",
        "        \"\"\"\n",
        "        urls = []\n",
        "        try:\n",
        "            root = ET.fromstring(rss_content)\n",
        "\n",
        "            # Поиск ссылок в разных форматах RSS\n",
        "            for item in root.findall('.//item'):\n",
        "                link = item.find('link')\n",
        "                if link is not None and link.text:\n",
        "                    urls.append(link.text)\n",
        "\n",
        "            # Альтернативный поиск\n",
        "            if not urls:\n",
        "                for link in root.findall('.//{http://www.w3.org/2005/Atom}link'):\n",
        "                    href = link.get('href')\n",
        "                    if href:\n",
        "                        urls.append(href)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка парсинга RSS: {e}\")\n",
        "\n",
        "        return urls\n",
        "\n",
        "    async def parse_article_page(self, url: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Парсинг страницы статьи\n",
        "\n",
        "        Args:\n",
        "            url: URL статьи\n",
        "\n",
        "        Returns:\n",
        "            Dict: Данные статьи или None\n",
        "        \"\"\"\n",
        "        html = await self.fetch_url(url)\n",
        "        if not html:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "        try:\n",
        "            # Извлечение заголовка\n",
        "            title = self._extract_title(soup)\n",
        "\n",
        "            # Извлечение текста статьи\n",
        "            content = self._extract_content(soup)\n",
        "\n",
        "            # Очистка текста\n",
        "            cleaned_content = self.text_cleaner.clean_html(content)\n",
        "\n",
        "            # Проверка валидности\n",
        "            if not self.text_cleaner.is_valid_article(cleaned_content):\n",
        "                logger.debug(f\"Статья невалидна, пропускаем: {url}\")\n",
        "                return None\n",
        "\n",
        "            # Извлечение даты публикации\n",
        "            published_at = self._extract_published_date(soup)\n",
        "\n",
        "            # Извлечение комментариев и рейтинга\n",
        "            comments_count = self._extract_comments_count(soup)\n",
        "            rating = self._extract_rating(soup)\n",
        "\n",
        "            # Подсчет слов\n",
        "            word_count = self.text_cleaner.count_words(cleaned_content)\n",
        "\n",
        "            # Извлечение категории\n",
        "            category = self._extract_category(soup)\n",
        "\n",
        "            return {\n",
        "                'title': title,\n",
        "                'description': cleaned_content,\n",
        "                'url': url,\n",
        "                'published_at': published_at,\n",
        "                'comments_count': comments_count,\n",
        "                'rating': rating,\n",
        "                'source': self.source_name,\n",
        "                'word_count': word_count,\n",
        "                'category': category\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при парсинге статьи {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Методы для переопределения в конкретных парсерах\n",
        "    def _extract_title(self, soup: BeautifulSoup) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _extract_content(self, soup: BeautifulSoup) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _extract_published_date(self, soup: BeautifulSoup) -> Optional[str]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _extract_comments_count(self, soup: BeautifulSoup) -> int:\n",
        "        return 0\n",
        "\n",
        "    def _extract_rating(self, soup: BeautifulSoup) -> Optional[float]:\n",
        "        return None\n",
        "\n",
        "    def _extract_category(self, soup: BeautifulSoup) -> Optional[str]:\n",
        "        return None\n",
        "\n",
        "    async def parse_site(self, rss_url: str, max_articles: int = 1000) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Основной метод парсинга сайта\n",
        "\n",
        "        Args:\n",
        "            rss_url: URL RSS ленты\n",
        "            max_articles: Максимальное количество статей\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Список статей\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "\n",
        "        try:\n",
        "            # Загрузка RSS\n",
        "            logger.info(f\"Загрузка RSS: {rss_url}\")\n",
        "            rss_content = await self.fetch_url(rss_url)\n",
        "\n",
        "            if not rss_content:\n",
        "                logger.error(f\"Не удалось загрузить RSS: {rss_url}\")\n",
        "                return articles\n",
        "\n",
        "            # Получение ссылок на статьи\n",
        "            article_urls = self.parse_rss(rss_content)\n",
        "            logger.info(f\"Найдено {len(article_urls)} статей в RSS\")\n",
        "\n",
        "            # Ограничение количества статей\n",
        "            article_urls = article_urls[:max_articles]\n",
        "\n",
        "            # Парсинг каждой статьи\n",
        "            for i, url in enumerate(article_urls, 1):\n",
        "                logger.info(f\"Парсинг статьи {i}/{len(article_urls)}: {url}\")\n",
        "\n",
        "                article = await self.parse_article_page(url)\n",
        "\n",
        "                if article:\n",
        "                    articles.append(article)\n",
        "                    logger.info(f\"Статья добавлена: {article['title'][:50]}...\")\n",
        "                else:\n",
        "                    logger.warning(f\"Не удалось распарсить статью: {url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при парсинге сайта {self.source_name}: {e}\")\n",
        "\n",
        "        return articles"
      ],
      "metadata": {
        "id": "F0gA3_DQl_mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Пример конкретного парсера parser/ria_parser.py:"
      ],
      "metadata": {
        "id": "Vwp5q57zmB_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from .base_parser import BaseNewsParser\n",
        "\n",
        "class RiaParser(BaseNewsParser):\n",
        "    def __init__(self):\n",
        "        super().__init__(source_name=\"ria\", base_url=\"https://ria.ru\")\n",
        "\n",
        "    def _extract_title(self, soup: BeautifulSoup) -> str:\n",
        "        # Поиск заголовка\n",
        "        title_tag = soup.find('h1', class_='article__title')\n",
        "        if not title_tag:\n",
        "            title_tag = soup.find('h1')\n",
        "\n",
        "        if title_tag:\n",
        "            return title_tag.get_text(strip=True)\n",
        "\n",
        "        # Поиск в мета-тегах\n",
        "        meta_title = soup.find('meta', property='og:title')\n",
        "        if meta_title:\n",
        "            return meta_title.get('content', '').strip()\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _extract_content(self, soup: BeautifulSoup) -> str:\n",
        "        # Поиск основного контента\n",
        "        article_body = soup.find('div', class_='article__body')\n",
        "        if not article_body:\n",
        "            article_body = soup.find('article')\n",
        "\n",
        "        if article_body:\n",
        "            return str(article_body)\n",
        "\n",
        "        # Альтернативный поиск\n",
        "        content_div = soup.find('div', {'itemprop': 'articleBody'})\n",
        "        if content_div:\n",
        "            return str(content_div)\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _extract_published_date(self, soup: BeautifulSoup) -> Optional[str]:\n",
        "        # Поиск даты в мета-тегах\n",
        "        meta_date = soup.find('meta', property='article:published_time')\n",
        "        if meta_date:\n",
        "            date_str = meta_date.get('content', '')\n",
        "            try:\n",
        "                # Преобразование в ISO формат\n",
        "                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "                return dt.isoformat()\n",
        "            except:\n",
        "                return date_str\n",
        "\n",
        "        # Поиск в тексте\n",
        "        date_tag = soup.find('div', class_='article__info-date')\n",
        "        if date_tag:\n",
        "            date_text = date_tag.get_text(strip=True)\n",
        "            try:\n",
        "                # Парсинг различных форматов даты\n",
        "                patterns = [\n",
        "                    r'(\\d{2})\\.(\\d{2})\\.(\\d{4})',\n",
        "                    r'(\\d{4})-(\\d{2})-(\\d{2})',\n",
        "                ]\n",
        "\n",
        "                for pattern in patterns:\n",
        "                    match = re.search(pattern, date_text)\n",
        "                    if match:\n",
        "                        if len(match.groups()) == 3:\n",
        "                            day, month, year = match.groups()\n",
        "                            if len(year) == 4:\n",
        "                                dt = datetime(int(year), int(month), int(day))\n",
        "                                return dt.isoformat()\n",
        "\n",
        "                return date_text\n",
        "            except:\n",
        "                return date_text\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_comments_count(self, soup: BeautifulSoup) -> int:\n",
        "        # Поиск количества комментариев\n",
        "        comments_tag = soup.find('a', class_='article__comments-link')\n",
        "        if comments_tag:\n",
        "            text = comments_tag.get_text(strip=True)\n",
        "            numbers = re.findall(r'\\d+', text)\n",
        "            if numbers:\n",
        "                return int(numbers[0])\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def _extract_rating(self, soup: BeautifulSoup) -> Optional[float]:\n",
        "        # Поиск рейтинга (если есть)\n",
        "        rating_tag = soup.find('div', class_='article__rating')\n",
        "        if rating_tag:\n",
        "            text = rating_tag.get_text(strip=True)\n",
        "            numbers = re.findall(r'\\d+\\.?\\d*', text)\n",
        "            if numbers:\n",
        "                return float(numbers[0])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_category(self, soup: BeautifulSoup) -> Optional[str]:\n",
        "        # Поиск категории\n",
        "        category_tag = soup.find('a', class_='article__tags-link')\n",
        "        if category_tag:\n",
        "            return category_tag.get_text(strip=True)\n",
        "\n",
        "        # Поиск в хлебных крошках\n",
        "        breadcrumbs = soup.find('nav', class_='breadcrumbs')\n",
        "        if breadcrumbs:\n",
        "            links = breadcrumbs.find_all('a')\n",
        "            if len(links) > 1:\n",
        "                return links[-1].get_text(strip=True)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "ha4LQZAAmN4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Файл main.py:"
      ],
      "metadata": {
        "id": "xpcYfjJ1mOwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Добавление пути для импорта модулей\n",
        "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "from config import NEWS_SITES, DB_PATH, MIN_ARTICLES_COUNT\n",
        "from database.db_manager import DatabaseManager\n",
        "from utils.rate_limiter import RateLimiter\n",
        "\n",
        "# Импорт парсеров\n",
        "from parser.ria_parser import RiaParser\n",
        "from parser.lenta_parser import LentaParser\n",
        "from parser.gazeta_parser import GazetaParser\n",
        "from parser.kommersant_parser import KommersantParser\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('news_parser.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class NewsParserApp:\n",
        "    def __init__(self):\n",
        "        self.db_manager = DatabaseManager(DB_PATH)\n",
        "        self.parsers = {\n",
        "            'ria': RiaParser(),\n",
        "            'lenta': LentaParser(),\n",
        "            'gazeta': GazetaParser(),\n",
        "            'kommersant': KommersantParser(),\n",
        "        }\n",
        "\n",
        "        # Проверяем существование базы\n",
        "        self.check_database()\n",
        "\n",
        "    def check_database(self):\n",
        "        \"\"\"Проверка состояния базы данных\"\"\"\n",
        "        try:\n",
        "            count = self.db_manager.get_article_count()\n",
        "            logger.info(f\"Текущее количество статей в базе: {count}\")\n",
        "\n",
        "            if count >= MIN_ARTICLES_COUNT:\n",
        "                logger.info(f\"Достигнуто минимальное количество статей ({MIN_ARTICLES_COUNT})\")\n",
        "            else:\n",
        "                logger.info(f\"Необходимо добавить еще {MIN_ARTICLES_COUNT - count} статей\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при проверке базы данных: {e}\")\n",
        "\n",
        "    async def parse_single_site(self, site_config, max_articles_per_site=500):\n",
        "        \"\"\"Парсинг одного сайта\"\"\"\n",
        "        parser_name = site_config['parser_class'].lower().replace('parser', '')\n",
        "\n",
        "        if parser_name not in self.parsers:\n",
        "            logger.error(f\"Парсер для {site_config['name']} не найден\")\n",
        "            return []\n",
        "\n",
        "        parser = self.parsers[parser_name]\n",
        "        logger.info(f\"Начинаем парсинг сайта: {site_config['name']}\")\n",
        "\n",
        "        try:\n",
        "            articles = await parser.parse_site(\n",
        "                site_config['rss_url'],\n",
        "                max_articles=max_articles_per_site\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Сайт {site_config['name']}: получено {len(articles)} статей\")\n",
        "\n",
        "            # Сохранение статей в базу\n",
        "            saved_count = 0\n",
        "            for article in articles:\n",
        "                article['source'] = site_config['name']\n",
        "                if self.db_manager.insert_article(article):\n",
        "                    saved_count += 1\n",
        "\n",
        "            logger.info(f\"Сайт {site_config['name']}: сохранено {saved_count} статей\")\n",
        "\n",
        "            # Закрытие сессии парсера\n",
        "            await parser.close_session()\n",
        "\n",
        "            return articles\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при парсинге сайта {site_config['name']}: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def parse_all_sites(self):\n",
        "        \"\"\"Парсинг всех сайтов\"\"\"\n",
        "        all_articles = []\n",
        "\n",
        "        for site_config in NEWS_SITES:\n",
        "            articles = await self.parse_single_site(site_config)\n",
        "            all_articles.extend(articles)\n",
        "\n",
        "            # Проверяем, достигли ли нужного количества\n",
        "            current_count = self.db_manager.get_article_count()\n",
        "            if current_count >= MIN_ARTICLES_COUNT:\n",
        "                logger.info(f\"Достигнуто минимальное количество статей ({MIN_ARTICLES_COUNT})\")\n",
        "                break\n",
        "\n",
        "        return all_articles\n",
        "\n",
        "    def print_statistics(self):\n",
        "        \"\"\"Вывод статистики\"\"\"\n",
        "        total_count = self.db_manager.get_article_count()\n",
        "\n",
        "        logger.info(\"\\n\" + \"=\"*50)\n",
        "        logger.info(\"СТАТИСТИКА ПАРСИНГА\")\n",
        "        logger.info(\"=\"*50)\n",
        "        logger.info(f\"Всего статей в базе: {total_count}\")\n",
        "        logger.info(f\"Минимально требуемое количество: {MIN_ARTICLES_COUNT}\")\n",
        "\n",
        "        if total_count >= MIN_ARTICLES_COUNT:\n",
        "            logger.info(\"✅ ТРЕБОВАНИЯ ВЫПОЛНЕНЫ\")\n",
        "        else:\n",
        "            logger.info(\"❌ ТРЕБОВАНИЯ НЕ ВЫПОЛНЕНЫ\")\n",
        "            logger.info(f\"Необходимо добавить еще {MIN_ARTICLES_COUNT - total_count} статей\")\n",
        "\n",
        "        logger.info(\"=\"*50)\n",
        "\n",
        "    async def run(self):\n",
        "        \"\"\"Основной метод запуска\"\"\"\n",
        "        logger.info(\"Запуск парсера новостных сайтов...\")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Парсинг всех сайтов\n",
        "            await self.parse_all_sites()\n",
        "\n",
        "            # Вывод статистики\n",
        "            self.print_statistics()\n",
        "\n",
        "            # Пример получения статей по источнику\n",
        "            ria_articles = self.db_manager.get_articles_by_source('ria', limit=5)\n",
        "            if ria_articles:\n",
        "                logger.info(\"\\nПример статей из РИА Новости:\")\n",
        "                for i, article in enumerate(ria_articles[:3], 1):\n",
        "                    logger.info(f\"{i}. {article['title'][:50]}... ({article['word_count']} слов)\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"Парсинг прерван пользователем\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Критическая ошибка: {e}\")\n",
        "        finally:\n",
        "            # Закрытие всех сессий\n",
        "            for parser in self.parsers.values():\n",
        "                await parser.close_session()\n",
        "\n",
        "            end_time = datetime.now()\n",
        "            duration = end_time - start_time\n",
        "            logger.info(f\"Парсинг завершен. Время выполнения: {duration}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Точка входа\"\"\"\n",
        "    app = NewsParserApp()\n",
        "\n",
        "    # Проверяем, нужно ли запускать парсинг\n",
        "    current_count = app.db_manager.get_article_count()\n",
        "\n",
        "    if current_count >= MIN_ARTICLES_COUNT:\n",
        "        logger.info(\"База данных уже содержит достаточное количество статей\")\n",
        "        app.print_statistics()\n",
        "    else:\n",
        "        # Запускаем асинхронный парсинг\n",
        "        asyncio.run(app.run())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "E7ob3rltmRU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Файл README.md:"
      ],
      "metadata": {
        "id": "gKRRzGhimTGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Парсер новостных сайтов\n",
        "\n",
        "Проект для сбора новостных статей с русскоязычных сайтов и сохранения их в базу данных SQLite.\n",
        "\n",
        "## Цель проекта\n",
        "\n",
        "Собрать корпус новостных статей (не менее 5000 записей) с различных русскоязычных новостных сайтов для последующего анализа данных.\n",
        "\n",
        "## Структура проекта\n"
      ],
      "metadata": {
        "id": "eOLxP93ymWI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "news_parser/\n",
        "├── parser/ # Модули парсеров для разных сайтов\n",
        "├── database/ # Работа с базой данных\n",
        "├── utils/ # Вспомогательные утилиты\n",
        "├── config.py # Конфигурация\n",
        "├── main.py # Основной скрипт\n",
        "└── requirements.txt # Зависимости"
      ],
      "metadata": {
        "id": "GSYg6iN7mYOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Требования\n",
        "\n",
        "- Python 3.8+\n",
        "- Установленные зависимости из `requirements.txt`\n",
        "\n",
        "## Установка\n",
        "\n",
        "1. Клонируйте репозиторий:\n",
        "```bash\n",
        "git clone <repository-url>\n",
        "cd news_parser"
      ],
      "metadata": {
        "id": "1cI72kllmczw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Установите зависимости:\n",
        "\n",
        "bash\n",
        "pip install -r requirements.txt\n",
        "Настройте конфигурацию в config.py:\n",
        "\n",
        "Добавьте/измените сайты для парсинга\n",
        "\n",
        "Настройте ограничения запросов\n",
        "\n",
        "Укажите путь к базе данных\n",
        "\n",
        "Использование\n",
        "Запуск парсера:\n",
        "bash\n",
        "python main.py\n",
        "Проверка базы данных:\n",
        "python\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Количество статей\n",
        "cursor.execute('SELECT COUNT(*) FROM articles')\n",
        "print(f\"Всего статей: {cursor.fetchone()[0]}\")\n",
        "\n",
        "# Статистика по источникам\n",
        "cursor.execute('''\n",
        "    SELECT source, COUNT(*) as count\n",
        "    FROM articles\n",
        "    GROUP BY source\n",
        "    ORDER BY count DESC\n",
        "''')\n",
        "print(\"\\nСтатистика по источникам:\")\n",
        "for row in cursor.fetchall():\n",
        "    print(f\"{row[0]}: {row[1]} статей\")\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "2Kq-AeH0mllb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Особенности реализации\n",
        "1. Поддержка сайтов\n",
        "РИА Новости (ria.ru)\n",
        "\n",
        "Лента.ру (lenta.ru)\n",
        "\n",
        "Газета.ру (gazeta.ru)\n",
        "\n",
        "Коммерсантъ (kommersant.ru)\n",
        "\n",
        "ТАСС (tass.ru)\n",
        "\n",
        "РБК (rbc.ru)\n",
        "\n",
        "2. Ограничения и вежливый парсинг\n",
        "Rate limiting (максимум 2 запроса в секунду)\n",
        "\n",
        "Случайные User-Agent\n",
        "\n",
        "Экспоненциальная задержка при повторных попытках\n",
        "\n",
        "Обработка HTTP-ошибок\n",
        "\n",
        "3. Очистка текста\n",
        "Удаление HTML-тегов\n",
        "\n",
        "Удаление медиа-контента (видео, аудио, изображения)\n",
        "\n",
        "Удаление скриптов и стилей\n",
        "\n",
        "Нормализация пробелов и переносов строк\n",
        "\n",
        "4. База данных\n",
        "SQLite с таблицей articles\n",
        "\n",
        "Поля: guid, title, description, url, published_at, comments_count, created_at_utc, rating, source, word_count, category\n",
        "\n",
        "Индексы для оптимизации запросов\n",
        "\n",
        "Поля базы данных\n",
        "Поле\tТип\tОписание\n",
        "guid\tTEXT\tУникальный идентификатор (UUID v4)\n",
        "title\tTEXT\tЗаголовок статьи\n",
        "description\tTEXT\tОчищенный текст статьи\n",
        "url\tTEXT\tСсылка на статью (уникальная)\n",
        "published_at\tTEXT\tДата публикации (ISO формат)\n",
        "comments_count\tINTEGER\tКоличество комментариев\n",
        "created_at_utc\tTEXT\tВремя создания записи в БД (UTC)\n",
        "rating\tREAL\tРейтинг/оценка статьи\n",
        "source\tTEXT\tИсточник (название сайта)\n",
        "word_count\tINTEGER\tКоличество слов в статье\n",
        "category\tTEXT\tКатегория/рубрика"
      ],
      "metadata": {
        "id": "KFsqupLAmshf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Создайте новый класс парсера в parser/:\n",
        "\n",
        "python\n",
        "from .base_parser import BaseNewsParser\n",
        "\n",
        "class NewSiteParser(BaseNewsParser):\n",
        "    def __init__(self):\n",
        "        super().__init__(source_name=\"newsite\", base_url=\"https://newsite.ru\")\n",
        "\n",
        "    def _extract_title(self, soup):\n",
        "        # Реализация извлечения заголовка\n",
        "        pass\n",
        "\n",
        "    # ... остальные методы\n",
        "Добавьте сайт в конфигурацию config.py:\n",
        "\n",
        "python\n",
        "NEWS_SITES = [\n",
        "    # ... существующие сайты\n",
        "    {\n",
        "        \"name\": \"newsite\",\n",
        "        \"base_url\": \"https://newsite.ru\",\n",
        "        \"rss_url\": \"https://newsite.ru/rss\",\n",
        "        \"parser_class\": \"NewSiteParser\"\n",
        "    }\n",
        "]\n",
        "Добавьте парсер в main.py:\n",
        "\n",
        "python\n",
        "from parser.newsite_parser import NewSiteParser\n",
        "\n",
        "self.parsers = {\n",
        "    # ... существующие парсеры\n",
        "    'newsite': NewSiteParser(),\n",
        "}\n",
        "Примеры использования данных\n",
        "Анализ тональности:\n",
        "python\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Анализ тональности статьи\n",
        "text = \"Текст статьи для анализа\"\n",
        "blob = TextBlob(text)\n",
        "sentiment = blob.sentiment\n",
        "print(f\"Тональность: {sentiment.polarity}, Субъективность: {sentiment.subjectivity}\")\n",
        "Частотный анализ слов:\n",
        "python\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Подсчет частоты слов\n",
        "words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "word_freq = Counter(words)\n",
        "print(\"Самые частые слова:\", word_freq.most_common(10))"
      ],
      "metadata": {
        "id": "gww2xhRHmu0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Объяснение последовательности действий:\n",
        "\n",
        "### 1. Выбор сайтов:\n",
        "Я выбрал 6 популярных русскоязычных новостных сайтов с разнообразным контентом:\n",
        "- РИА Новости (государственные новости)\n",
        "- Лента.ру (общие новости)\n",
        "- Газета.ру (общественно-политические новости)\n",
        "- Коммерсантъ (деловые новости)\n",
        "- ТАСС (официальные новости)\n",
        "- РБК (экономические новости)\n",
        "\n",
        "### 2. Организация парсинга:\n",
        "- Использован объектно-ориентированный подход\n",
        "- Базовый класс `BaseNewsParser` содержит общую логику\n",
        "- Для каждого сайта создан свой класс-наследник\n",
        "- Используется асинхронный парсинг для эффективности\n",
        "\n",
        "### 3. Учет ограничений:\n",
        "- Rate limiting через класс `RateLimiter`\n",
        "- Случайные User-Agent для обхода блокировок\n",
        "- Экспоненциальная задержка при повторных попытках\n",
        "- Ограничение частоты запросов (2 запроса в секунду)\n",
        "\n",
        "### 4. Очистка текста:\n",
        "- Удаление HTML-тегов через BeautifulSoup\n",
        "- Удаление медиа-элементов (видео, аудио, изображения)\n",
        "- Удаление скриптов и стилей\n",
        "- Нормализация текста\n",
        "\n",
        "### 5. Работа с базой данных:\n",
        "- SQLite для простоты развертывания\n",
        "- Автоматическая генерация GUID\n",
        "- Индексы для оптимизации запросов\n",
        "- Проверка уникальности по URL\n",
        "\n",
        "### 6. Особенности реализации:\n",
        "- Использование RSS для получения списка статей\n",
        "- Парсинг полной страницы для получения контента\n",
        "- Валидация статей (минимальная длина, наличие текста)\n",
        "- Статистика и мониторинг процесса парсинга\n",
        "\n",
        "Этот проект позволяет собрать более 5000 статей с различных источников, обеспечивая при этом вежливый парсинг и качественную очистку данных."
      ],
      "metadata": {
        "id": "GCCOSZl0nEMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}